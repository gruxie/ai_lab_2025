
<a id="top"></a>
**Workshop 1:  Study preparation**
=================================
  
Goals:
- Exercise the prompts from the first module from end to end.
- Exercise re-working prompts and dialogue.
- Experiment with working collectively on a single project. 

Format of the workshop (5 min)
- We will cover the prompts from three of our previous lab sessions:  literature review, hypothesis generation and JTBD hypothesis generation.  
- We will work individually on the same research question so we can compare results after each section
- The group gets to choose the research question from among three options.  
- Materials (papers, books) will be provided for you - you are free to seek out additional papers, books if you like.
- We have 15 min for each section.  Each step will have a  basic outline
  - Setup (3 min)
  - Lab guide (instructions) (7 min)
  - Group reflection (5 min)  
  
   
>[!NOTE]
>These prompts were tested in ChatGPT o1.  It's recommended that you use the best possible LLM (with the most reasoning power) available.  The exercises are not sensitive and are appropriate to use with any public LLM.

#### Tools and materials you will need
- This guide
- Prompts - links to prompts are included in each section but you can download them all (.zip) here.
- LLM - any LLM of your choosing (ChatGPT, Claude, Gemini)
- Collaboration board - An online whiteboard for sharing and discussion.  We will capture information in [Mural](https://app.mural.co/t/customerdrivenengineering9401/m/customerdrivenengineering9401/1742226675471/baf7a8942174198f3428c3572512c56e6b1144f2?sender=ude733ec3e37feb743ac63134)  
 

### Prompt Troubleshooting: <a id="ts"> </a>
- **Shallow results?** - additional prompts to get more out of the results:
  - Re-run the prompt - see if it produces better results; try an alternate LLM (e.g. Claude)
  - Add context:  what did you hope or expect to see but didn't?
  - Ask the AI to reflect on itself; have it think about their results for the literature it's produced and recommend improvements on either its results or your prompt.  
  - Ask the AI to refresh it's results - for example:
    - include key findings from the [article of your choice] that relate to my research question.
    - Include direct quotes from the [article of your choice]
- **Off track?**  
  - Review the output for anchors or the term that took the LLM off track and re-word
  - Explain what you wanted to see but didn’t and ask it to incorporate your feedback and re-generate its results.

 
Quick Nav:  [BibBibliography](#section1.1) | [Hypothesis Gen.](#section1.2) | [JTBD Hypothesis Gen](#section1.3) | [Prompt troubleshooting](#ts) | [Top](#top)

___
<br><br>














## <a id="section1.1"></a>Section 1:  Annotated literature review (15 min)

### Section 1 Setup (3 min)    
**Big picture:**  this prompt is meant to give you a starting point when you are new to the material.  The expectation is that you would find articles or materials through this process that you would later evaluate more deeply in order to learn and improve your confidence in the LLM's results.  This is not intended to replace learning about your subject area!

>[!IMPORTANT] Reminders:  AI can hallucinate.  Think of it as a launching-off point (not a final destination).  Double check findings as you proceed and remain skeptical.  

### Section 1 Instructions (7 Min)  


1.  Choose your domain area - you will use this for the entire lab!   
2.  Run the prompt for the domain area you've chosen for the workshop (open in new tab).      
    - [Lit Review - Option 1](https://google.com) 
    - [Lit Review - Option 2](https://google.com)  
    - [Lit Review - Option 3](https://google.com)  
3.  Think aloud!
4.  Evaluate the results:  
    1.  Identify 2 or more insights from the literature review that are interesting and relevant to your research question. Add them to the Mural board!
5. Dialogue:  Choose one paper and drill down on key findings through dialogue.  For example…
   1. What hypotheses could we draw from [this paper] that are relevant to my research question?
   2. What other research questions does [this paper] raise that are relevant to my research question? 
6.  Add one or more of the results of the dialogue into the "Section 1" group of the Mural board.  

### Section 1 Reflection questions (5 min)  
- What, if anything, did you learn that was new to you about this topic?
If this was our project How do we feel about the quality of these results?
- What concerns do you have about results so far?
- What would make the results better?
<br><br>  
Quick Nav:  [BibBibliography](#section1.1) | [Hypothesis Gen.](#section1.2) | [JTBD Hypothesis Gen](#section1.3) | [Prompt troubleshooting](#ts) | [Top](#top)
<br><br>













## <a id="section1.2"></a> Section 2:  Hypothesis generation (15 min)

### Section 2 Setup (3 min)  

**Big picture:** This prompt will generate a list of assumptions and hypotheses (using the HPF format) based on the context set in the literature review built in <a id="section1.1"> Section 1</a>.  

>[!IMPORTANT]
>This prompt requires continuity from the previous step (literature review) for its context. Before running this prompt, ensure you have completed the previous step in Section 1.  You will run this prompt in the SAME chat session!


### Section 2 Instructions (7 Min)  

1.  Complete the exercise from [Section 1](#section1.1) if you haven't already. 
2.  Keep the chat session with your results from Section 1 open.  You will use it for the rest of this exercise.
3.  Open the the hypotheses generation prompt for the research domain you have chosen:       
    - [Hypothesis generation - Option 1](https://google.com) 
    - [Hypothesis generation - Option 2](https://google.com)  
    - [Hypothesis generation - Option 3](https://google.com) 
4.  Make any adjustments or edits you would like to the "References section" that includes the research question, job performer, and context for the study. The defaults built into the prompt will work fine - but you have the option to change them. 
5.  Run the prompt!  It will run in two steps:
    1.  It will generate assumptions first.  When it pauses at this step, you can add assumptions of your own to the LLM's context window. 
    2.  After telling the LLM to proceed, it will use the assumptions created in the previous step to generate a list of hypotheses
6.  Evaluate the results
7.  Review the hypotheses that the LLM generated:
    1.  Did it produce the hypotheses that were asked for in the prompt?  
    2.  Are they in the right HPF format?
    3.  Select the hypotheses that is most interesting to you and add it to the mural board.
8.  Dialogue:  choose one of the hypotheses that was generated in the previous step and ask the LLM to create more related hypotheses.  "Expand on hypotheses [use an identifier from the output].  generate 4-5 related hypotheses 













### Section 2 Reflection questions (5 min)  
- How useful or distinct were the NEW hypotheses (generated in step 8)?  If they need repair, what would you change about the output?
- Based on what you know, how 'testable' do these hypotheses seem to you?
If this was our project How do we feel about the quality of these results?
- If you were running this study on your own, what next steps would you take in study planning?  How would you use these results, if at all?

<br><br>  
Quick Nav:  [BibBibliography](#section1.1) | [Hypothesis Gen.](#section1.2) | [JTBD Hypothesis Gen](#section1.3) | [Prompt troubleshooting](#ts) | [Top](#top)
<br><br>



## <a id="section1.3"></a>Section 3:  JTBD hypothesis generation (15 min)
### Section 3 Setup (3 min): 
**Big picture:**  This prompt will generate a list of main jobs-to-be-done and supporting data (e.g. desired outcomes, sub jobs) for a given job performer.  It also produces recommended questions for a study guide.
  
>[!IMPORTANT]
>Like previous sections, the scaffold of the prompt is provided but in this section, participants will have to make the final edits .  

### Section 3 Instructions (7 Min)  

1.  Refer back to the output of the first two sections (literature review and hypothesis generation) and determine how to update this prompt:
    1.  What job performer should you focus on?
    2.  What research question should use?  This last question should be a refinement of the research question used so far.  The prompt uses the RQ as context for producing it's output.  
2.  Open the the JTBD prompt for the research domain you have chosen:       
    - [JTBD Analysis - Option 1](https://google.com) 
    - [JTBD Analysis - Option 2](https://google.com)  
    - [JTBD Analysis - Option 3](https://google.com) 
3.  In the "#ASSUMPTIONS" section of the prompt, apply your edits to the "Research Question(s)" and "Customer or Job performer.  For the sake of the workshop, limit both to one RQ and one job performer.  Use this step to make any additional changes to the "Context" section that you prefer.
4.  Run the prompt!  It will run all of the analysis in one step.
5.  Evaluate the results
    1.  Review the list of jobs-to-be done and desired outcomes.  Does the outcome statement match the intended form (Direction of change + unit of measure + clarifier?
    2.  Are the results scoped to narrowly?  Did it anchor on a specific idea in the research question?  What happens when you remove the research question and re-run?
    3.  Pick one of the jobs that is most familiar or interesting to you.  Review the desired outcomes and type (emotional or functional).  Does the classification ring true?  Add the Job, the outcome, and your feedback to the mural board.
6. Dialogue:  Review the results from Phase 3 (Job Steps) and identify a job that was NOT selected from Phase 1 (Extract Jobs).  Ask the LLM to generate job steps for the job that you selected that WASN'T in Phase 3.
        1.  Ask the LLM to explain it's reasoning to you in language a high school student would understand.  
        2.  Ask it to reflect on it's analysis of the job-to-be-done and to make recommendations on how to improve it's output.

### Section 3 Reflection questions (5 min)
1.  How challenging was it to edit a prepared prompt like this?
2.  Which is more useful for learning a new domain: Main Jobs, desired outcomes or job steps?

<br><br>  
Quick Nav:  [BibBibliography](#section1.1) | [Hypothesis Gen.](#section1.2) | [JTBD Hypothesis Gen](#section1.3) | [Prompt troubleshooting](#ts) | [Top](#top)
<br><br>

END OF WORKSHOP 1
